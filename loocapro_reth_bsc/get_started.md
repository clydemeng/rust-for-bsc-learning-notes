
# run a full node
By default: 
data file : /Users/ruojunm/Library/Application Support/reth/bsc/db


## cmd
10W blocks
``` sh
RUST_LOG=info ./target/release/reth-bsc node \
--chain bsc --http --http.api="eth, net, txpool, web3, rpc" --metrics 0.0.0.0:9001 \
--debug.tip 0x4270b97b8a4bea1edd1f3523d691342fd2002d5066cd8b4667bfbee245380046 \ 

--db.max-size 7TB
```
100W blocks


``` sh
RUST_LOG=info ./target/release/reth-bsc node \
--chain bsc --http --http.api="eth, net, txpool, web3, rpc" --metrics 0.0.0.0:9001 \
--debug.tip 0xe468563fd42441b615ef6132474b2b6692fea05c7523356b79b8e5a414909360 \ 
```


500W blocks

``` sh
RUST_LOG=info ./target/release/reth-bsc node \
--chain bsc --http --http.api="eth, net, txpool, web3, rpc" --metrics 0.0.0.0:9001 \
--debug.tip 0xc47ffcf02770b9510df200e30811e915772770658cac69af9bd40df97dba1cb6 
--log.file.directory ./target/release/logs

```






## db path
/Users/ruojunm/Library/Application Support/reth/bsc



# As we don't have Parlia engine  inside the loocapro_reth_bsc,  how is the block syncing performance, comparing to the case when we have a parlia engine? 

Todayâ€™s â€œdebug-tipâ€ workflow and a future genuine Parlia-engine workflow share the same downloader, database schema and REVM execution; the differences are in HOW and WHEN the pipeline can mark blocks canonical and therefore how efficiently stages overlap.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. What happens _now_ ( `--debug.tip` mode )
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ You inject a single synthetic `forkchoiceUpdated` pointing at some tip hash.  
â€¢ The header downloader walks **backwards** from that tip to genesis; no block can be
  marked canonical until all ancestors are present.  
â€¢ Bodies / Execution / Hashing / Merkle start **only when** their predecessor stage
  reaches the bottom of the range; they cannot run ahead because the canonical chain is
  not updated incrementally.  
â€¢ When you raise the tip later you repeat the whole backward walk from the new hash.

Net result  
â€£ Good peak bandwidth for headers (â‰ˆ15â€“25 k headers / s over a single peer) but poor
  stage parallelism.  
â€£ Sync throughput on commodity hardware tends to be in the 2â€“4 k **blocks** per second
  range for the first few million blocks, then slows because each restart requires a
  fresh header walk.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2. What changes with a real ParliaEngine
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Every new header received from peers is **immediately** verified (signature,
  proposer rotation) and inserted into `CanonicalHeaders`.  
â€¢ The pipeline can run in its intended mode:
  â€£ `Header` stage works on the **future** tip,  
  â€£ `Body` stage chases a little behind,  
  â€£ `Execution` & hashing stages trail further but never stop; checkpoints move
    continuously.  
â€¢ No need to restart with a new `--debug.tip`; the CL keeps feeding heads.

Extra cost  
â€¢ Signature verification per block (ğ‘‚(1 k) Âµs with Rust `secp256k1` on modern CPUs).  
â€¢ Validator-set updates every epoch (small DB writes).  
Both are negligible compared to REVM execution time.

Net result seen in other Parlia implementations  
â€¢ Header download: unchanged (network-bound).  
â€¢ Body download + execution now overlap, so overall **wall-clock sync speed doubles or
  triples**.  
  â€“ On a 16-core Ryzen box, BSC mainnet can sync ~60â€“80 k **blocks per minute** once
    Parlia is in place; thatâ€™s â‰ˆ3â€“4Ã— faster than the staged debug-tip approach.  
â€¢ After it catches up, the node continues at live-chain pace without manual input.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Summary
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Debug-tip mode = convenient for testing but introduces a sequential bottleneck;  
  Real ParliaEngine = small CPU overhead, big win in end-to-end catch-up time and no
  manual restarts.



  # How does pipeline stage works

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Where is the code?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ The entire stage-and-pipeline engine lives in **upstream reth**:  
  `reth/crates/stages/*` and `reth/crates/node_core/*`.  
â€¢ The BSC fork (`loocapro_reth_bsc`) only _wires_ those building-blocks together in  
  `src/node/mod.rs` and swaps in BSC-specific builders (consensus, executor, etc.).  
  Nothing inside `loocapro_reth_bsc` re-implements the stage logic.

Key files to skim:

```20:93:loocapro_reth_bsc/src/node/mod.rs
ComponentsBuilder::default()
    .pool(EthereumPoolBuilder::default())
    .executor(BscExecutorBuilder::default())
    .payload(BscPayloadServiceBuilder::default())
    .network(BscNetworkBuilder { â€¦ })
    .consensus(BscConsensusBuilder::default())
```

```65:120:reth/crates/stages/stages/src/sets.rs      // simplified
HeaderStage   â”€â”   (online)
BodyStage     â”€â”¤
SenderRecovery â”¤
Execution      â”¤
AccountHashing â”¤
StorageHashing â”¤   (offline)
MerkleStage    â”¤
TxLookupStage  â”¤
IndexHistory   â”¤
PruneStage     â”¤
FinishStage    â”€â”˜
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. High-level workflow
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. **Node start-up** (`reth-bsc node`)
    a. `NodeBuilder` instantiates all *components*:  
      network, payload builder, executor, consensus, storage.  
    b. It then constructs a `Pipeline` that owns an _ordered list of stages_.  
        â€¢ The list comes from `DefaultStages` in reth (see file above).

2. **HeaderStage (reverse first)**
    â€¢ Finds a pivot hash (in `--debug.tip` mode this is your CLI hash).  
    â€¢ Downloads headers **backward** until they meet the DB checkpoint.  
    â€¢ As soon as a contiguous range exists, it writes those headers to
      `CanonicalHeaders`.

3. **BodyStage (forward)**
    â€¢ Requests transaction/ommer bodies for every header whose body is missing,
      starting at the _last_ body checkpoint.  
    â€¢ Validates block hashes vs downloaded bodies; fills `Transactions`,
     `BlockBodyIndices`, etc.

4. **SenderRecoveryStage**
    â€¢ Recovers `from` addresses from each txâ€™s signature and stores them in
     `TxSenders`.

5. **ExecutionStage**
    â€¢ Uses REVM (wrapped in `BscExecutorBuilder`) to execute blocks in forward
     order, writing state, receipts, bloom, etc.

6. **Hashing & Merkle stages**
    â€¢ Re-hash modified accounts / storage keys.  
    â€¢ Build Merkle-Patricia trie nodes and update the state root.

7. **Indexing & Pruning stages**
    â€¢ Build aux indices (tx-lookup, history).  
    â€¢ Optionally prune segments according to the nodeâ€™s `--prune` config.

8. **FinishStage**
    â€¢ Dummy stage signalling the pipeline reached the configured target.

Each stage implements the `Stage` trait (`execute`, `unwind`, `checkpoint`) located
in `reth_stages_api::Stage`.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. What changes once you add a real Parlia engine?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Nothing in the stage _code_; only the **source of the pivot** changes:

â€¢ Today (`--debug.tip`) the pivot is fixed and headers must reach genesis before the
 pipeline can progress.

â€¢ With a Parlia engine the pivot moves automatically; as soon as Parlia validates a
 header, it becomes canonical and downstream stages can proceed in parallel, giving
 you big speed-ups.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. Which repo to read?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ For understanding the pipeline mechanics: **read the reth code** (files above).  
â€¢ For seeing how BSC plugs into it: inspect the small builder modules in
 `loocapro_reth_bsc/src/node/*`â€”they merely adapt rethâ€™s generics.

Use this map as your guide when you deep-dive: headers & bodies in `HeaderStage`,
execution in `ExecutionStage`, hashing/merkle in their respective stages, all
under `reth/crates/stages/â€¦`.




## loocapro_reth_bsc/src/main.rs

what is "sender"?
what is "receiver"?
can you explain more fo rthe dependency loop? I do not notice any loop.


Below is a â€œguided reading mapâ€ that answers the three questions and shows what source files to open next.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1.  oneshot Sender vs Receiver
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ â€œoneshot channelâ€ = asynchronous mailbox that delivers **exactly one value**.  
  â€“ `let (tx, rx) = oneshot::channel::<T>();`

```
   Sender<T>  (tx) â”€â”€ send(T) â”€â”€â”€â”€â”€â–¶  Receiver<T>  (rx)  â”€â”€ .await  â”€â”€> T
```

â€¢ After `send()` succeeds the channel closes; further sends error out.  
â€¢ This is ideal for boot-strapping when
  â€“ component A must start **now**, but
  â€“ it canâ€™t do real work until component B hands it some object later.

In your snippet, `T = BeaconConsensusEngineHandle<BscPayloadTypes>`.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2.  Why was the one-shot needed?  (the dependency loop)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Letâ€™s zoom in on the two components:

```
BscNetworkBuilder  â”€â”€â”€ needs â”€â–¶  BeaconConsensusEngineHandle
        â–²                                 â”‚
        â”‚                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€  built by  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

â€¢ The **network** service must know how to talk to the consensus engine
  (it forwards `newPayload` / `forkchoiceUpdated` messages).  
â€¢ The **consensus engine** canâ€™t exist until `NodeBuilder::launch()` finishes,
  because it needs a ready DB, executor, etc.

If we tried to hand the handle directly weâ€™d have circular construction.  
A one-shot breaks the circle:

```
(1) BscNode::new()               â”€â”€> gives Network an Arc<Mutex<Option<Receiver>>>
(2) NodeBuilder::launch() creates the engine        \
(3) main()    engine_handle_tx.send(handle);         > one value hops across
(4) Network receives the handle and continues      /
```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3.  Arc::<T>::new and why Mutex<Option<Receiver>>
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ `Arc<T>` â€“ thread-safe reference counting so multiple tasks can own the same
  `Receiver`.  
â€¢ `Mutex`   â€“ only one task may `take()` the `Receiver`, preventing a race.  
â€¢ `Option`  â€“ after one component extracts the receiver with `take()`, the
  field becomes `None` so subsequent attempts know the value is already used.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4.  Code-reading roadmap after `main()`
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Main path:

1. **`src/main.rs`**  
   â€“ Parses CLI, builds a `reth_cli::CliBuilder`, calls `.node(node).launch()`.

2. **`reth/crates/node_core::NodeBuilder::launch()`**  
   â€“ Assembles all **Components** (network, pipeline, RPC, etc.).  
   â€“ Starts the **Pipeline** task that drives the stages.

3. **`reth/crates/stages/stages/src/sets.rs`**  
   â€“ `DefaultStages` lists the exact stages (HeaderStage, BodyStage, ExecutionStage â€¦)  
   â€“ Each stage implements the `Stage` trait (`execute` + `unwind`) in its own file
     under `reth/crates/stages/stages/src/stages/`.

4. **Stage files to open next** (follow the sync flow):
   â€¢ `header.rs`   â€“ reverse header download algorithm.  
   â€¢ `body.rs`     â€“ body downloader & validation.  
   â€¢ `sender_recovery.rs`  
   â€¢ `execution.rs` (wraps REVM).  
   â€¢ `account_hashing.rs`, `storage_hashing.rs`, `merkle.rs`.

Skim them in that order; each fileâ€™s `execute()` function is annotated with what it
does and which DB tables it writes.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
5.  Recap in one sentence
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
`BscNode::new()` uses a one-shot channel (Sender / Receiver) wrapped in
`Arc<Mutex<Option<_>>>` to hand the consensus-engine handle to the
network component _after_ the node is fully launched, thus avoiding a
construction dead-lock and letting you keep the whole setup fully
asynchronous.



## 